LUN 2 JUIN
  -visite locaux
  -rdv claire matthieu
    -définition des objectifs du stage : 
      -cmprendre l'algo de marche aléatoire,
      -recoder l'algo (en particulier pour les spanning trees) s'il est simple
      -voir sur un département en particulier (correze je crois) les découpages qui sont générés et s'ils sont particuliers
      -calculer l'inégalité avec la taille des coupes et voir sie elle est tight et pas du tout
      -voir en pondérant les noeuds : comment trouver le spanning tree : uniforme ?
      -voir en pondérant les arêtes (on ne veut pas avoir une trop grande frontière)
  -visite biblio
  -recupération des fichiers
  
  A FAIRE : 
  -installer latex
  -regarder si le code julia fonctionne
  -améliorer le code juila pour écrire les formes une seule fois et les colorier plutot que de les dessiner beaucoup de fois!
  
  
 MAR 3 JUIN
 
   -lecture de l'article de DeFord, Duchin et Solomon (58pages), compréhension de la marche aléatoire utilisée
   -installation de julia et latex
   -tentative (échec) de faire fonctionne le code de decoupageColorieTex.jl
   
   A FAIRE
    -comprendre le code de decoupageColorieTex.jl OK
    -vérifier l'installation de latex
    
MER 4 JUIN
  -installation de latex sans soucis
  -ok pour julia
  -lecture d'articles sur l'algo de wilson (en particulier celui cité dans le papier de Recom) : étrange car différent de ce que attendu (bon finalement c'est différent de ce que David avait dit mais tranquilou pour lui c'est la mm chose, peut être) : tentative de comparaison avec le module Recom utilisé
  RDV DAVID : 
   -utilisent spanning tree avec poids aléatoires avec kruskal -~ distribution non uniforme, pas l'objectif principal de ce qu'on veut faire
   OBJECTIF(S) : 
    -aller voir o`u est l'inégalité voulue
    -coder l'algo de sampling de découpage avec wilson pour ensuite pouvoir avoir la distribution et pouvoir faire des tirages aléatoires pour estimer la distibution en fonction de paramêtres qu'on définira plus tard
    -idée possible : faire la même chose en utilisant l'algo
  
  A FAIRE
   -trouver o`u est la génération de spanning tree dans le module pour trouver quel algo est utilisé, wilson ou non : ils ont utilisé wilson dans le passé, mais utilisent un kruskal avec des poids aléatoires (ce qui leur permet d'ajouter des facteurs sur ces poids pour favoriser le choix de telle ou telle arête)
   -faire un .sh pour générer les dessins coloriés (en fait non, il suffit de passer en param les fichiers (en modifiant les 0x en x si x plus petit que 10)
   
JEU 5 JUIN
  -début du code de Recom
  -
  
  A FAIRE : 
   -problème dans la condition de partition en deux circo de l'unification de deux circos : cf feuille.Il faut modifier ça, sinon on arrive à un truc qui génère des partitions bizarres. (UPDATE 15 JUIL : je ne comprends pas ce que je veux dire ici)
   -vérifier qu'il n'y a pas un risque de ne pas atteindre certains découpages ie que a et b soient des découpages valides et que pour tout "chemin" entre a et b il y ait un c qui ne soit pas valide, ie que cela crée une sorte de barrière
   
VEN 6 JUIN
  -crise : il faut faire les affiches du R4T (en plus arrivée en retard bref)
  matin : affiches
  
 
 
LUN 9 JUIN : Férié

MAR 10 JUIN 
  -arrivée assez tard (11h)
  -repartie assez tot (15h30)
  
  RDV DAVID : 
   - ok de
  OBJECTIFS : 
   -générer des cartes selon la distribution Recom (1000 itérations) et voir ce que ca donne : 
     -est ce qu'il y a des cartes non atteintes (quelle est la distribution (uniforme ?))
     -voir par rapport à l'inégalité donnée par la taille de la coupe (cf un papier envoyé/dans le sujet de stage)
     -voir si augmenter de 1,2, 50, 100 change qqch à la dist, si réduire d'e 1, 2, 100,500 change qqch ?
     -voir si changer de configuration initiale change quelque chose ou non
     -réfléchir à des mesures pour quantifier les découpages
     
MER 11 JUIN
  -tentative de modification du programme pour générer les découpages selon la dist de recom
  -problème : comment avoir l'indice de présence d'un découpage dans le tableau (fct très mal)
  
  obj aprem : faire un système tq dans un découpage écrit dans un fichier : la première circo qui apparait sera notée : 0, deuxième circo qui apparait 1 ... jusqu'à la dernière circo : recherche linéaire (en le nombre de cantons) et ça c'est cool : il faut modifier les dichiers de sortie de David pour avoir ça 
  -on a reussi à faire un fichier qui comporte les découpage et leur nombre d'occurence d'apparition, reste à savoir l'afficher
  -le truc de la recherche linéaire a été fait, mais pas la modif des fichier de DS, on les recalcule à chaque lecture de ses fichiers
  
  A FAIRE : 
   -faire l'affichage des découpage en fonction de leur proba d'apparition
   -faire les "vraies" stats -~ a priori impression que ça se groupe surtout sur certains découpages 
   -voir par rapport à l'inégalité de David
   
JEU 12 JUIN
  -test de la distribution sur différentes instances (dpt 9, 50) avec différents découpages de départ, en générant 100 samples avec 1000 itérations chacun -~ on voit bien une idée de proba inversement prop à la taille de la coupe -~ à checker !
  
  
  
  -on est quand même sacrément sur une inégalité asymptotique : comment faire pour la comparer avec de petites valeurs ? -~ il y a une ineg dans landau dans le fichier : ok
  
VEN 13 JUIN 
  - tentative de connexion ssh
  - tentative d'implémentation de kruskal : si utilise uf sans completion des chemins, obtient t-on un arbre valide ?
  
  RDV DAVID
   -les samples semblent se concentrer sur les découpages ayant une petite coupe, on se demande si ça ne se concentre pas justement trop sur les petites coupes, ce qui donnerait que la distribution ne serait pas vraiment selon la taille de la coupe (enfin je pense que si, juste que l'inégalité ne serait pas tight, ce qui est une chose qu'on voulait évaluer par l'expérience!) . En effet, selon l'inégalité, s'il y a un facteur 2 entre deux tailles de coupes, alors la proba doit etre 4 fois plus grande (environ) (et pas beaucoup plus que 4 sinon ça fait tout capoter, à vérifier -~ ici c'est l'idée que l'inégalité ne serait pas tight (ou au moins pas tight partout))
   -la distance euclidienne n'est pas forcément la bonne, cf distance de géographes entre deux points = le nombre de personnes entre ces deux points (avec un cylindre ou non, pas clair) globalement : comment le généraliser pour les frontières
   -idée d'utiliser kruskal avec du bruit (mon idée)
   -récupération des fichiers
   -NB : on ne peut pas utiliser la distance euclidienne, il faut une distance ORTHODROMIQUE (géodésique plutot je pense) (vérifier) (ie qui prenne en compte la rondeur de la terre)
   -Exemple de département qui a des frontières très différentes : doubs (notamment il y a la ville et des cantons avec une forme très étrange) (mais en général ont beaucoup de découpages différents)
   
   OBJECTIFS : 
   -histogramme avec la taille de la coupe en abscisse et en ordonné le nb découpage et la proba d'apparition (peut être des colonnes avec plusisuers couleurs, une pour chaque découpage)
   -voir comment ajouter la pondération des arêtes
   -voir à quel point il y a un lien entre nombre de spanning tree et taille de la coupe
   -déssiner les graphes plutot que les département pour avoir une idée en regardant la topologie du graphe
   -possibilité de faire un tirage avec une idée de duplication d'arêtes proportionnelle à la taille de l'arête en question : ça nous donne un tirage selon la distribution du produit des poids (ie plus un arbre a un produit de ses poids élevés, plus il aura de chances d'être tiré)
   -regarder s'il y a des choses qui ne sont pas coupées ou si par le découpage avec recom (exemple : communautés de communes (fleuves ?))
   algo de kirchoff et theorème des matrices de ...pour calculer le nombre de spanning tree
   
   
LUN 16 JUIN : 
 -création des histogrammes du nombre d'apparition dans la génération avec Recom
 -avancé sur la génération de spanning tree avec kruskal et les poids aléatoires
 
 RDV CLAIRE & David (visio)
  -explicarion du rdv de vendredi
  -idée selon laquelle le cas à deux circos dans un dpt doit être connu totalement, en tout cas la distribution
  
 A FAIRE 
  -créer les diagramme cardinal de la coupe en abscisse, nombre de spanning tree en ordonnée
  -essayer de comprendre parfaitement le cas à deux circos dans un dpt
  -faire plus d'itérations (100 c'est pas assez) pour voir si les proba d'apparition sont les même pour une taille de coupe donnée
  NB : pour le cas de 2 circos, pas besoin de faire x itérations, 1 suffit, car on refusionne à chaque fois 
  
  
MAR 17 JUIN
 -coder le calcul du nombre de spannng forest en fonction d'un découpage
 -automatisation pour créer les fichiers sortie_numDep_bis.txt avec le cardinal de la coupe et le nombre de spanning forest 
 -dessin de courbes avec les valeurs : interessant (on dirait une exponentielle)
 
 Obj pour l'aprem : 
   -OK modifier le code pour que le graphique soit plus beau (avoir deux axes verticaux, avoir des légendes tout ça)
   -faire la même chose pour l'histogramme d'hier (ajouter le nombre de découpages sur le coté droit)
   -analyser les données mises en lumières par ces graphiques
   -faire le graphique pour tous les départements à 5% si rapide non?
   -Problème de foret couvrante dans les territoires non connexes non?
   
   -finalement j'ai été à un séminaire sur les H-free graphs : pas eu le temps de faire beaucoup de choses....
   
   
   
MER 18 JUIN

  OBJECTIFS : 
   -générer des données (ou bien le faire pendant la nuit, à voir xD)
    (générer des samples, les tableaux associés, et les deux histos)
    histo nb_spanning tree en fonction de taille coupe -² OK
    (générer les fichiers avec les tailles de coupe et les nombres de spanning tree) OK
   -utiliser le serveur de calcul de l'irif
   
   
  13H30 : RDV Claire Matthieu IRIF (+ david en visio normalement)
    -montré les graphiques de nb spanning tree en fonction du card de la coupe
    -on peut mettre en log l'ordonnée pouravoir une belle droite c'est cool
    -discussion de la structure des arêtes qui peuvent couper dans l'arbre : c'est un arbre (cf sur papier je l'ai écrit)
    -on veut avoir la choix de l'arête qui coupe en ayant l'arbre de wilson en O(n) (alors que le faisait en O(n²) avant (n = taille de l'arbre) (pour cela faire des tableaux tq chaque noeud coco son fils et son père et son/ses frères
    
    OBJECTIFS :
    -faire des calculs avec le serveur de calcul de l'irif
    -réfléchir à la structure des arêtes de l'arbre
    -améliorer l'algo en conséquence
    -faire les graphiques nombres d'occurence du découpage en fonction du nombre de spanning tree
    -penser à répondre à david s'il envoie un mail pour envoyer au géographe 
    -voir si c'est toujours vrai en faisant ça sur des graphes à 20% plutot que 5%
    -calculer la constante de l'article avec l'inégalité
    -calculer les valeurs de K_1 et k_2 pour nos graphes pour pouvoir tester les inégalités sans trop de problèmes
    -faire un graphique qui donne la valeur de l'inégalité en abscisse et en ordonnée
    
    
JEU 19 JUIN 
  Questions : comment représenter graphiquement l'inégalité ? car dépend de deux découpages à la fois : il faudrait un graphique en 3D qui aurait en abscisse les deux découpages considérés et en ordonné le quotient des proba et en même temps la partie minorant l'inégalité... -~ pas fou à représenter
  OU bien en 2D en jouant sur la couleur des points pour avoir la distance (ie partir vers le bleu pour si différence positive, rouge sinon et tendre vers le noir s'il y a un gros écart (sinon tourner vers un plus clair mais pas non plus du blanc quoi)
 
  
  
  
  -légère erreur sur le code de recom je pense (le truc pour partitionner les arbres plus efficacement) mais bon il faudra chercher quoi....
  COMMENT ON OBTIENT LE GRAPHE DUAL JE SUIS PERDUE -~ truc avec un ondonnancement des arêtes en fonction de l'angle et compter à partir de ça
  
  
VEN 20 JUIN 
  -reflexion sur le graphe dual  comment l'obtenir ? (idée de cycles à partir de la mat adj OU complétion des arêtes du fichier .out pour faire le graphe des intersections mais on obtient des noeuds en plus je pense

  13h30 ou 14h : RDV Claire Matthieu ! (sans david)
    CF mail pour david et claire du vendredi 20 juin pour explication
    
    NB : idée du dual qui peut avoir des multis arêtes 
    
 LUN 23 JUIN
   -vérifier l'équivalence entre l'intersection de plus grande arité = nb faces du graphe dual : je pense que la structure pour stocker le graphe et son dual simultanément donne une preuve assez immédiate OK
   -calcul de k_2 de manière pratique (ie modif l'algo pour) OK
   
   NB : possibilité d'erreur dans l'algo si deux intersections sont reliées par un unique segment : n'arrive essentiellement jamais je pense
   
   -calcul de l'inégalité raffinée dans programme : valeurs non réeelement concluentes (pas assez de sampling)
   OBJ de l'après midi : 
    -utiliser le serveur de l'irif pour faire des calculs !  OK (ne fonctionne pas tout le temps a priori)
    -faire un bon gros readme pour expliquer les fichiers tout ça car je m'embrouille (et il y a plusieurs versions différentes) (OK, ça avance)
    -il faudrait peut etre ajouter le nombre total de samples dan ke ficheir tableau_distribution .... 
    
MAR 24 JUIN 
  -utilisation du ssh pour faire des calculs : c'est très long, donc il faudrait optimiser pour avoir un calcul beaucoup plus rapide !!
  -sur dpt 21 (cote d'or (ou cote d'armor jsp)) calcul (5 circos) de 100 samplings avec 1000 itérations chacunes, on va voir ce que ça va donner ! 
  
  -étude de la preuve du lemme pour vendredi 
  
  
  RDV Claire et David (en visio)
   -montré les graphiques avec la différence entre coté gauche et coté drit de l'équation -~ modification en quotient plutot que différence
   
   OBJECTIFS : 
    -comprendre la preuve de l'inégalité pour vendredi 27
    -essayer de faire des graphiques coté droit en ord, coté gauche en abscisse et différence de tailles de coupe en couleur
    -commencer à réfléchir aux mails de jean jsp-qui et la fille (annie ou marie ou bref)
    -regarder si on peut avoir une majoration plutot que seulement une minoration : voir avec la fct inverse : normalement on obtient bien une majoration : vérifier (mais en vrai bizarre car borne trop faible selon moi mais bon) -~ non car on a une contrainte sur les tailles de coupes
    
MER 25 JUIN 
  -deuxième lecture de la preuve de l'inégalité (pendant toute la matinée, la productivité n'est pas wtf)
  
  Obj de l'aprem : regarder ce qu'on peut faire avec des frontières pondérées, pour préparer le rdv de vendredi
   sinon je peux aussi réparer l'amélioration de l'algo qui utilise la structure d'arbre des arêtes qui coupent
   ou bien je peux aussi faire des calculs en tout genre sur le serveur de l'irif
   
JEU 26 JUIN : 
  -David a répondu à mon mail avec beaucoup de questions : objectifs : relire la démo et répondre aux questions de David
  -relire la démo à présenter vendredi OK 
  -faire l'implémentation pondérée 
  -lire la démo de effective resistance = proba d'être pris dans un unifomly random spanning tree (MOYEN)
  -envoyer mail récap du dernier rdv (avec claire et David en visio au début)
  -faire les trucs sur la courbe log de la frontière : pour ça il faudra probablement faire des calculs assez nombreux
  -réparer le deuxième histogramme qui n'affiche que les axes et pas les données ce qui est quand même assez dommage
  
  
VEN 27 JUIN 
  -relire la preuve que je dois présenter à 14h
  
  -faire faire des calculs au serveur de l'irif (sur dpt à découapges uniquement à 20%)
  
  
  RDV avec Claire et David (visio) de 14h à 14h30 pour présenter la preuve de l'inégalité intéressante
    -ils ont constaté que la preuve était plus technique que ce qu'ils pensaient, donc deux possibilité : soit abandonner, soit y passer pas mal de temps pour avoir une bonne idée de ce qui se passe derrière la preuve (algèbre (linéaire) des graphes)
    -finalement la preuve est bien appliquable, il suffit de prendre alpha grand
    
  
  RDV avec Claire, David et au moins un géographe de 14h30 à 15h30 pour discuter de ce qu'est la compacité
    -le géographe semblait un peu à l'ouest, a pensé qu'on n'avait pas d'algo qui samplait avec des poids alors qu'on peut facilement adapter recom
    
  Objectifs : 
    -la semaine prochaine David est en vacances donc seule claire est là
    -ca peut être bien de calculer les pentes des courbes en log et de comparer entre les départements
    -attention, peut etre utile de ne pas utiliser sur chaque dpt mais de choisir un certain sous ensemble pour plus tard
    -j'aimerais boucler la partie code ie faire des trucs tels que on ait un truc intéressant et homogène (d'un pt de vue conventions) niveau code
      -donc il faudrait coder : 
        -l'opti avec la structure d'arbre pour trouver les arêtes qui coupent
        -la marche aléatoire avec des arêtes pondérées
    -peut être faire le graphique en mettant le coté droit de l'équation en abs, le coté gauche en ordonnée, et mettre la taille de la coupe en couleur pour observer la tête de l'inégalité
    -essayer de comprendre parfaitement le cas des départements à deux circonscriptions
        
        
  Conseil de david : ne pas hésiter à dire quand on ne comprends pas qqch
  
  
LUN 30 JUIN
  CODEEEE
  -avancée sur le code avec arêtes pondérées

  -j'ai l'impression que le random successeur n'est pas bon, ou que les probas sont trop différentes pour etre tirées car on peut tourner pendant très longtemps sans jamais trouver l'arbre en faisant wilson, en fait ce n'était pas ça, j'avais sorti un truc d'une boucle alors qu'il ne fallait pas, du coup ça avait tout cassé, mais maintenant tout va mieux
  
  
  -algo de génération avec Recom pondéré fonctionne et génère l'histogramme, néamoins le résultat n'est pas fifou
    -on ne génère pas particulièrement des cartes avec petites frontières, parfois on est même sur une sorte de gaussienne, ce qui n'est pas le résultat voulu.
    -on peut essayer de modifier le choix de l'arête qu'on prend parmi les arêtes qui coupent, mais dans ce cas pourra t-on encore décrire précisément la distrbution obtenue ??
    
  -j'ai fait le graphique coefficient de la courbe du log du nombre de spanning tree en fonction du cardinal de la coupe (avec une regression linéaire)
  on arrive à une moyenne de -0.95 - -0.97, médiane à peu près au même endroit, ça nous dit que globalement il n'y a pas de coef dans l'expo, on a bien  nb_spanning_tree = cste * exp( -1 * card_coupe)  -~ idée de la constante :oui, c'est exp(val en zéro) qu'on peut récupérer sans problème
    En regardant le log de la constante, on remarque qu'on a qqch d'assez chaotique, on ne peut globalement pas en dire grand chose ....
    
    
MAR 1 JUIL
  Canicule : venue à l'irif le matin
  -la mesure de la taille de la coupe ne semble pas bonne pour le cas des arêtes pondérées, je pense qu'il faut plutot prendre le produit/la somme des poids des arêtes dans les circos 
    -somme : 
    -produit : 
    -quotient (produit taille circos / taille coupe) :
    on a l'impression que toutes ces mesures donnent les mêmes graphiques, qui ne représentent pas qqch d'interessant
    
    ces mesures ne donnent pas des résultats concluents.... pas fou
    
    
  -Je pense qu'il y a un truc étrange avec les probabilités quand on fait l'arbre de wilson, car on s'autorise à faire des allers-retours sur une même arête, ce qui je pense ne correspond pas à ce qu'il faut : après réflexion non, (ne pas oublier qu'on ne relie pas chaque feuille à la racine mais bien à l'arbre. NB : le fait d'autoriser des retours pourrait poser un problème de convergence, mais dans les cas pratiques qu'on a on n'a pas ce problème puisque nos poids sont plus ou moins equivalents je pense (on n'a pas 1Million face à 2 quoi)
  Cela dit on peut peut être optimiser en évitant d'aller à un endroit qui va nous obliger à revenir ? (apres il faut checker que ce ne soit pas la racine, à voir ...) (pas fait, peut être un jour mais pas maintenant) 
  voir avec plus de samples et plus de tours si ça change qqch, mais j'en doute (pour le nombre de tours, pour le nombre de samples probablement)
  
  idée d'autre mesure : prendre un truc comme la taille moyenne d'une arête dans une circo (?) ou le quotient de la taille moyenne d'une arete dans une circo sur la taille moyenne d'une arête dans la coupe
  le truc c'est que dès qu'on fusionne deux circos pour les re-séparer on introduit une "grosse" arete dans la coupe, ce qui n'est pas fou. On a donc nb_circ * (nb_circ - 1) /2 "grosses" aretes dans la coupe, et certes c'est négligeable quand il y a 2-3 circos, mais quand il y en a 5 ou plus c'est la cata (enfin tout dépend la taille de la coupe, mais typiquement pour le dpt 21 abvec 5 circos et un card de coupe de en moyenne 30-35 ça fait 1/3 de "grosses" arêtes dans la coupe.....
  
  Reflexion à un truc qui pourrait permettre de ne pas introduire une grosse arête à chaque fois qu'on fusionne et recoupe deux partition, à base de ajouter une petite arête dans l'arbre pour créer un cycle et trouver un arête qui en l'enlevant fera que la petite arête coupera
  est ce qu'on ne va pas couper les mêmes arêtes ?? bah en fait je pense que si, ça se voit bien avec un simple cycle est ce qu'on ne peut pas en faire qqch quand même, je pense qu'on ne peut rien en faire, c'est dommage 
  
  
MER 2 JUIL 
  -RDV Claire 10h :
    parler des coefs des données du nombre de spanning tree en fonction du card de la coupe, de la cste qui varie 
    parler de l'algo pondéré : les distibutions que ça donne (dpt 12 et 21)
DEBUT RESUME

- nb STs en fct card coupe (non pondéré):
  j'ai calculé les valeurs du coefficient directeur de la courbe (je l'appelle m) qui semblait apparaître dans le graphique log(nb st) en fonction du cardinal de la coupe, on obtient des valeurs entre _0.9 et -1.10 globalement
  j'ai fait la même chose pour les valeurs de l'ordonnée à l'origine de cette même droite (je l'appelle p), valeurs dépendant du département, peut varier du simple (~20) au double. N'a pas l'air de dépendre de la valeur de m
  j'ai calculé les valeurs des coefficients de corrélations, on est globalement tout le temps sur du -0.99 (deux dpts à 0.98.), donc corrélation "assurée"


  Questions :
    Que peut on dire des départements qui ont des valeurs particulières pour m et/ou p ?
    Regarder s'il y a un lien particulier avec le nb de circos (2?) ou la taille de la coupe par ex


Algo en lui même :
  Questions :
    Comme on comprends globalement ce qui se passe dans les dpts avec 2 circos, il faudrait étudier ceux avec 3 circos (en prendre un en particulier, et essayer de comprendre la distribution)
    Il faudrait aussi regarder s'il y a de la connexité dans cette histoire, ou des découpages qui sont très difficilement atteignables à partir de certains autres
    Dessiner des graphiques du nombre d'occurrence dans la distribution en fonction du nombre de ST pluton que en fct du card de la coupe

-Algo pondéré :
  -je l'ai implémenté, et en générant des distributions sur certains départements (surtout dpts 12 et 21, resp 3 et 5 circos) j'ai des résultats assez peu concluent (un découpage n'est pas tiré proportionnellement à la taille (au sens somme des poids des arêtes) de la coupe

  Questions :
   le lien entre taille de la coupe (au sens somme ..) et taille des STs (somme) est elle vraie comme dans le cas non pondéré ? OKKK
   Sinon, on peut faire les graphiques sur la proba d'apparition en fonction de la taille des STs (ou somme des tailles, à voir)


L'algo est probablement bizarre, car en cas de card de coupe de 20-30 et 5 circos, on peut arriver à avoir 1/2 ou 1/3 "grosses" arêtes dans la coupe (issues des STs que l'on divise quand on coupe deux circos fusionnées) -~ il faudra y réfléchir (prendre exemple simple avec un dpt "linéaire", une arête centrale à 1000 et toutes les autres à 1, on sq on peut couper n'importe quelle arête, alors la distribution ne sera pas valide (on devrait ne quasiment jamais couper à l'arête de poids 1000))

FIN RESUME

l'après midi j'ai vu Olivier et joué à Hanabi

JEU 3 JUIL
  -matin jusqu'à 11h30  :j'ai été à l'IHP pour un workshop pour les 60 ans de claire mathieu
  
  - trouver s'il y a un lien entre nb ST et card coupe en pondéré aussi :
    -pour cela on def le card de la coupe en pondéré comme l'analogue de la somme des poids des arêtes présentes dans la coupe
    -on def de la même manière le nb de ST comme la somme des poids des arêtes présentes dans les ST
    -donc avant on faisait les graphiques nb st en fonction de card coupe, maintenant on fera somme (sur les ST) de la somme des poids des arêtes dans le st considéré en fonction de la somme des poids des arêtes dans la coupe
      -est ce que cet analogue est bon? car si on applique ça au cas non pondéré, ie chaque arête a poids 1 : on aura la bonne abscisse, mais pas la bonne ordonnée car chaque arbre devrait peser 1 et non pas le nombre d'arêtes qu'il possède
       -on en arrive à se dire qu'il faut mettre en ordonnée pondérée la somme sur les ST de la somme des arêtes divisé par le nombre d'arêtes dans le ST considéré
       
       
  -finalement he suis repartie à l'anniversaire de Claire l'après midi pour rencontrer une personne qui a travaillé sur Cgal, voir si ça peut aider, mais honnêtement j'ai de gros doutes (car déjà c'est en dimension d donc c'est sortir un canon pour tuer une mouche) 
  donc je n'ai pas travillé de l'après midi
  
VEN 4 JUIL 
  -matin : rdv avec Ky pour les stages de m1, donc matinée assez courte
  sinon il faut calculer tous les ST sur le sous graphe induit par chaque circos, calculer les poids et ensuite faire le "produit" ie faire la somme pour tout nb_circ-uplet, donc il faut adapter mon algo pour des restrictions de graphes et pas uniquement tout le graphe -~ OK
    ou bien lui faire calculer toutes les st de toutes les circos d'un coupe -~ OK
    
  mon algo de génération des ST ne fonctionne pas comme je veux, je vais péter un boulon -~ OK


DIM 6 JUIL 
  -avancée sur l'algo pour générer tous les st (j'ai fait un truc avec des union find c'était bien)
  -j'arrive à générer les graphqiues (en tout cas ppur le dpt 21)
  
LUN 7 JUIL
  -on remarque que le poids des sts en fonction du poids de la coupe forme une ligne super epaisse -~ on n'a pas la belle ligne du cas non pondéré
  - la moyenne des poids des st en fonction du poids de la coupe donne un nuage de points : pas très pertinent....
  
  -j'ai mis en lumière quels étaient les dpts avec le plus grand / plus petit coef de correlation / cste, mais je ne sais pas quoi dire de ces dpts, mis à part le fait qu'il y ait beaucoup de petits cantons dans celui avec le plus petit (marne) (-1.10) coef de correlation par rapport à celui qui a le plus grand (charente) (-0.95)
  
  
Objectif du jour !
IL FAUT TROUVER COMMENT TOUT COPIER SUR VIM/EMACS !!! (pour transferrer du ssh vers mon pc) -~ OKKK
Objectif du jour ! 

J'ai regardé le maximum des poids des arbres couvrants pour une taille de coupe donnée -~ on a un truc plutot linéaire, ce qui n'est pas incroyable

RDV DAVID : (il m'a envoyé son résumé aussi)
  on a discuté de plusieurs trucs : 
  
  ST en fonction de Taille de coupe : 
    -on a le lien en non pondéré, mais pas en pondéré.
    -en gros m est important mais pas p, car on a P(apparition) = f(taille de la coupe) avec f linéaire, car on a f linéaire en le nombre de ST (car si f = k * nb_st, alors dans l'inégalité on a un quotient de nb_st et vu que nb_st = exp(card coupe) on a l'inégalité (j'ai quand même un doute sur cette histoire car on aurait un quotient d'expo de card de coupe alors qu'à coté on a une expo d'un quotient...)
  
  
  Algo non pondéré : 
   - on a l'impression que les samples vont beaucoup trop se concentrer sur les découpages avec petites coupes, on a envie de réduire la pente de la droite en échelle log pour que ça se concentre moins sur les petites coupes
   -sur l'inégalité : je ne m'en souvenais plus, mais en fait on ne peut pas faire l'inverse de l'inégalité par symétrie des roles car on a une hypothèse sur les card des coupes les unes par rapport aux autres ...
   -on doit faire plus de calculs avec plus de samples et plus de tours et le représenter en fonction du nombre de spanning tree et pas forcément en fonction du cardinal de la coupe (même si les deux sont reliés c'est pas foufou)
   
  
  
  Algo pondéré : 
   -on doit faire des graphiques avec le nombre de samples en fonction du nombre de spanning trees (et pas leurs poids)
   En effet, on a l'impression (cf truc alpha sur papier) que vu qu'on a un produit de deux trucs tels que la somme est fixée on maximise quand les deux sont à peu pres égaux, donc le but va être d'avoir beaucoup d'arbres couvrants, quand bien même ils seraient de poids assez faible
   -on doit essayer de ne pas tirer uniformémment l'arete à couper parmi les arêtes qui coupent, mais de prendre la plus petites parmi celles qui coupent
   -il faut vérifier la théorie selon laquelle il y a plusieurs choix d'arêtes possibles pour couper
   
   -regarder ce que fait wggg pour gérer les pondérations des arêtes 
   -regarder le th de Kirchoff = matrix tree thm (?)
   
   
   ATTENTION : vérifier que j'utilise bien des produits d'arêtes et pas des sommes !!!!!
   
   On voudrait trouver un moyen de séparer en deux deux circos fusionnées en prenant en compte la taille (au sens somme des poids) de la coupe, plutot que de prendre le nombre de st qui n'est plus relié au card de la coupe 
   
MAR 9 JUIL : 
  -refelxion sur comment réduire la pente de son truc exponentiel : on peut juste refuser le découpage avec proba très faible inversement proportionnel en la taille de la coupe 
  - lecture de la preuve de matrix tree théorem
  -choix des départements de tests pour faire les calculs
  - preparation pour faire les calculs sur le serveur, début des calculs
  
  
MER 10 JUIL : 
  -sur les dpt à 2 circos, il n'y a pas le choix des arêtes qui coupent 
  -avancée des calculs sur le serveur : c'est beaucoup plus long pour les dpt à 5% que ceux à 20%, donc c'est casse pieds
  -regardé comment mggg font pour gérer les pondérations : ils ne les gèrent pas : ils utilisent kruskal avec poids aléatoires qu'ils peuvent biaiser pour faire en sorte que telle ou telle arête soit prise dans l'arbre et donc soit mise avec tres faible proba dans la coupe (il y a une proba 1/taille arbre que celle ci soit dans la coupe (et encore il faut que l'arbre coupe, ce qui n'est pas sur)
  
  
JEU 10 JUIL 
  -discussion avec David dès 10h : déjà il y a un problème dans le matrix tree thm (ou avant d'y entrer car on arrive toujours à des puissances de 3, ce qui est quand même sacrément étrange) (UPDATE : en fait c'est normal, car les circos ont en général 3 cantons ou moins)
  -pour la compréhension à 3 circos : il faut surtout avoir une intuition et tenter de la prouver : par exemple prouver que le nomnbre de ST influe grandement
  -regarder si on a pkus d'arêtes qui coupent dans les ST générés pour des dpts qui ont des dec à 5% (regarder sue les arbres a 20%)
  -est ce que la dist est la même entre algo pondéré et non pondéré
  
  
VEN 11 JUIL : RDV David 11h

truc à raconter à david : 
  -pour rendre la pente de son truc exponentiel moins pentu : possibilité de changer de rejeter le découpage avec une proba (très faible) invesement proportionelle à la taille de la coupe, grande question : est ce qu'il faut le faire seulement à la fin de tous les tours ou à chaque étape de recom ? -~ la question sous-jacente étant est ce que avoir un découpage de petite coupe à un tour favorise les découpages à petites coupes au tour suivant ? ( a priori je pense que oui pour des raisons évidentes)
  - pour le truc des puissances de 3 : tout va bien c'est normal
  - pour le nombre d'arêtes dans la coupe : en général il n'y en a pas beaucoup, mais un peu quand même (il y en a beaucoup essentiellement quand on cherche les découpages à 20% sur des départements qui admettent des découpages à 5%)
    -ne sert à rien de comparer les poids de ces arêtes car ce qui importe c'est le poids de la coupe induite par l'arête en question, ce qui est assez long à calculer
  - est ce que la dist est la même avec pondération ou non (ie est ce que ce ne sont pas juste le nb d'arbres couvrants qui influent dessus) : en non pondéré on est quand même beaucoup plus concentré sur les arbres avec beaucoup de st que dans le cas pondéré. De plus pour un nombre de st donnée, les occurrences sont plus uniformes avec le cas non pond qu'avec le cas pond
  -j'ai regardé comment mggg gérait les pondérations : problème solved
  
  - A VOIR j'aimerais bien essayer de ne pas utiliser wilson mais d'utiliser kruskal avec du bruit car ça doit être plus intéressant d'un point de vue distribution -~ j'aimerais savoir si on tire beaucoup d'arbres couvrant avant d'en avoir un qui coupe
  - regarder si on peut adapter le matrix tree thm au cas pondéré -¬¬¬ si on l'adapte au graphes pondérés (en mettant -le poids de l'arête au lieu de -1 et la somme des arêtes connectées au sommet au lieu de son degré) on obtient la somme des poids des arbres, en ayant poids des arbres = produit des poids des arêtes qui le composent 
  
  
  à ajouter au résumé fait par email du rdv avec david : 
   ce serait bien de voir la vitesse de découpage sur nos départements 
   voir la distance entre deux distributions de proba pour savoir si on a atteint la dstribution stationnaire ou non
   regarder s'il y a d'autres algos que recom utilisés aux states
   
   
16 JUIL
  RDV DAVID et claire (cf photo du tableau sur téléphone)
    l'algo de kruskal est utilisé en pratique pour mggg, il peut y avoir plusieurs raisons à ça : le fait que ça permette de biaiser facilement ls pondérations pour les fleuves et autres (même si en pratique on peut aussi le faire avec Wilson) et pour la rapidité (c'est un peu plus rapide que wilson a priori).
    On remarque que la distribution attendue par kruskal n'est pas la distribution stationnaire, cela dit on ne connait pas réellement la distribution générée (avec un carré avec une diagonale on prouve que ce n'est pas la distribution uniforme). Des papiers affirment que les arbres générés par Kruskal sont de hauteur O(n) donc ce sont des arbres filiformes avec qq excroissances à droite à gauche
    
    Reflexion sur Wilson : on aimerait en savoir la rapidité : savoir quand on trouve un chemin qui percute l'arbre, savoir combien de nouveau sommets on a visités (enfin combien vont être ajoutés dans l'arbre quoi) -~ tout ca pour connaitre la rapidité de Wilson
    
    objectifs : 
     -Approche 1 : regarder pour faire la matrice de transition et Recom et calculer la distribution stationnaire de Recom
     -Approche 2 : faire du coupling from the past pour avoir la dist stationnaire
      Ces deux approches vont permettre de dire si c'est connexe ou pas (la première on va avoir un automate non connexe, et pour le deuxième ça va converger vers 2 trucs différents
      
    SANS POIDS, 2 CIRCOS : 
      -on comprend la distribution générée par l'algo (et le fonctionnement de l'algo en lui-même)
      -on comprend le lien entre la proba de tirer un découpage et (le nb_st et le card coupe) (qui sont reliés entre eux deux)
      -on aimerait savoir générer selon d'autres distributions que la distribution exponentielle -~ possible ? réduction vers le voyageur de commerce ou un truc avec un cycle hamiltonien
        -en particulier, est ce que générer une carte de manière uniforme selon le card de la coupe est NP-difficile (l'enemble probabiliste selon Divid de ce que je me souviens, à vérifier), toujours le cas dans le cas planaire (un "Jérome" aurait travaillé la dessus)
        
    AVEC POIDS, 2 CIRCOS : 
      -pas fou car on n'arrive pas à générer selon le paramètre voulu qu'est la taille de la coupe. 
      
    SANS POIDS, 3 CIRCOS : 
      -différence : on fait 1000 pas de Recom contre un pour les dpts à 2 circos, on arrive à qqch qui est peut etre la distribution stationnaire
      Exemple sur un dpt qui aurait 150 dec admissibles : on fait la matrice de transition qui est de taille 150*150 -~ calcule la dist stationnaire avec des vecteurs propres/valeurs propres ...
      Autre possibilité : avec le coupling from the past
      
      
      
    A faire pour vendredi 14h : 
      -avoir un plan de ùmon rapport de stage
      -vérifier ce qu'on pense de mes graphiques nb st en fct de nb_occ
      -commencer à faire une matrice de transition OU comprendre l'algo de coupling from the past
      
      
JEU 17 JUIL 
  -lecture du papier cftp (j'en avais déjà lu un hier mais j'etais trop fatiguée je n'ai rien compris du tout
  
  obj de l'aprem : 
  -implémenter cftp et voir les résultats obtenus -~ en cours
  -faire un plan de rapport pour apporter à David et Claire -~ en cours
  -quantifier (faire une distance de probas) entre  -les distributions générées en commençant par des découpages différents
  -faire des trucs avec Kruskal
  - !!!!!! liberer de l'espace sur le disque !!!!!!
  
  -J'ai fait le retour sur trace sur des départements : 
    -département 74 (6 circos) (28 dec à 20%) : on s'arête toujours sur un truc qui converge vers 5 découpages différents : on en conclut qu'il y a 5 CC
    -département 68 (6 circos) (138 dec à 20%) :
      [291, 193, 188, 123, 260, 161, 371, 404, 274, 227, 226, 223, 173, 257, 589, 109, 431, 196, 494, 737, 326, 288, 378, 413, 915, 273, 170, 127, 197, 402, 324, 369, 116, 336, 204, 202, 379, 101, 462, 351, 292, 256, 657, 452, 589, 198, 226, 571, 298, 292, 293, 360, 245, 180, 315, 548, 200, 161, 460, 124, 928, 424, 426, 107, 267, 71, 461, 230, 476, 285, 114, 536, 588, 313, 479, 265, 350, 141, 845, 346, 368, 428, 257, 590, 239, 524, 205, 224, 197, 559, 754, 208, 170, 157, 219, 262, 304, 392, 276, 500]
      (nb_etapes avant convergence pour 100 samples)
      
    
      
    -departement 10 (3 circos) (95 dec à 5%)
    (nb_etapes avant convergence pour 1000 samples)
      [54, 115, 37, 97, 40, 41, 68, 61, 95, 30, 43, 84, 41, 43, 61, 32, 64, 127, 77, 49, 19, 209, 47, 40, 29, 58, 41, 38, 112, 69, 38, 23, 27, 102, 76, 66, 41, 20, 52, 110, 28, 112, 49, 96, 38, 21, 58, 50, 34, 49, 40, 41, 78, 65, 76, 57, 33, 26, 58, 62, 45, 46, 148, 86, 68, 42, 154, 44, 62, 46, 42, 24, 110, 29, 47, 89, 47, 47, 53, 53, 64, 57, 39, 18, 80, 61, 39, 121, 42, 28, 69, 31, 66, 41, 40, 72, 33, 103,   171  , 70, 41, 41, 43, 99, 36, 68, 27, 51, 52, 37, 48, 64, 20, 30, 57, 29, 50, 44, 112, 62, 64, 41, 65, 37, 55, 47, 150, 29, 33, 45, 116, 42, 43, 46, 68, 38, 47, 67, 47, 76, 77, 50, 47, 33, 25, 47, 86, 81, 43, 81, 40, 62, 30, 60, 56, 32, 69, 46, 47, 36, 45, 25, 24, 75, 64, 31, 25, 56, 137, 85, 160, 93, 127, 50, 45, 103, 59, 37, 48, 39, 39, 38, 64, 29, 103, 66, 49, 62, 62, 38, 28, 42, 45, 25, 42, 68, 140, 45, 37, 48, 34, 79, 49, 81, 67, 110, 48, 50, 93, 20, 154, 77, 50, 42, 27, 79, 35, 70, 45, 76, 16, 43, 29, 59, 49, 50, 57, 43, 97, 40, 38, 31, 85, 39, 70, 27, 83, 72, 83, 40, 70, 66, 48, 103, 91, 41, 29, 61, 38, 48, 40, 41, 79, 114, 64, 86, 44, 71, 64, 76, 35, 73, 76, 48, 18, 33, 56, 63, 39, 62, 96, 29, 69, 24, 63, 52, 47, 55, 87, 26, 23, 70, 89, 45, 53, 39, 41, 50, 60, 48, 37, 57, 112, 76, 68, 69, 77, 87, 53, 46, 38, 46, 45, 38, 69, 62, 106, 64, 93, 31, 58, 35, 62, 34, 79, 49, 37, 35, 36, 28, 50, 108, 33, 55, 68, 52, 52, 98, 88, 105, 85, 112, 49, 41, 51, 105, 48, 71, 59, 44, 53, 42, 44, 40, 65, 63, 34, 29, 45, 47, 96, 78, 37, 87, 36, 51, 100, 59, 50, 40, 34, 39, 39, 84, 70, 25, 24, 46, 36, 72, 32, 40, 92, 42, 82, 62, 75, 78, 43, 44, 67, 40, 34, 31, 67, 46, 24, 40, 34, 51, 49, 34, 42, 46, 33, 99, 37, 37, 26, 55, 35, 29, 50, 156, 66, 46, 105, 62, 56, 33, 79, 82, 55, 68, 127, 164, 42, 29, 65, 123, 23, 27, 130, 36, 43, 58, 114, 38, 37, 47, 57, 45, 87, 67, 157  , 26, 69, 42, 17, 60, 86, 112, 102, 49, 47, 74, 55, 32, 53, 95, 49, 52, 33, 31, 45, 53, 120, 52, 34, 49, 53, 69, 72, 62, 47, 103, 29, 46, 77, 54, 87, 37, 47, 68, 60, 62, 59, 32, 49, 33, 57, 75, 60, 64, 30, 139, 42, 70, 39, 41, 37, 62, 54, 37, 39, 87, 84, 40, 27, 74, 87, 66, 77, 25, 82, 25, 68, 30, 31, 78, 29, 139, 64, 64, 53, 44, 44, 111, 29, 70, 33, 65, 83, 46, 93, 35, 54, 99, 82, 43, 76, 43, 27, 55, 42, 101, 43, 29, 112, 70, 43, 34, 57, 51, 76, 56, 93, 35, 86, 59, 71, 33, 21, 40, 28, 97, 56, 37, 58, 22, 50, 54, 116, 134, 67, 49, 34, 35, 77, 24, 46, 61, 55, 110, 56, 65, 34, 39, 42, 42, 85, 60, 37, 57, 49, 47, 30, 68, 48, 32, 31, 37, 68, 27, 14, 41, 49, 51, 78, 33, 128, 21, 41, 26, 53, 54, 46, 23, 86, 40, 42, 119, 44, 91, 46, 36, 27, 44, 28, 31, 38, 40, 52, 102, 89, 55, 49, 35, 39, 42, 36, 36, 90, 78, 85, 138  103, 63, 25, 47, 25, 102, 111, 45, 55, 74, 56, 30, 63, 35, 32, 62, 74, 50, 35, 82, 72, 66, 75, 27, 69, 21, 62, 117, 39, 24, 51, 45, 59, 86, 69, 24, 45, 32, 82, 95, 57, 36, 50, 66, 49, 46, 53, 55, 76, 62, 56, 35, 55, 75, 66, 41, 126, 39, 58, 37, 59, 86, 69, 40, 104, 22, 36, 34, 43, 34, 35, 62, 28, 67, 76, 46, 45, 54, 26, 38, 42, 40, 48, 24, 63, 37, 48, 43, 50, 55, 63, 47, 115, 93, 118, 99, 76, 41, 28, 27, 44, 26, 39, 104, 39, 36, 48, 41, 38, 101, 43, 58, 106, 37, 72, 58, 61, 28, 34, 89, 54, 39, 38, 65, 60, 38, 77, 83, 18, 42, 72, 54, 60, 24, 82, 55, 43, 74, 21, 141, 57, 81, 82, 48, 69, 89, 44, 85, 35, 81, 35, 56, 140, 53, 27, 35, 88, 52, 69, 30, 46, 55, 39, 107, 19, 25, 69, 51, 45, 38, 84, 27, 40, 39, 43, 66, 76, 50, 34, 31, 51, 60, 57, 52, 90, 42, 37, 80, 67, 24, 61, 138, 32, 79, 67, 62, 102, 40, 91, 48, 48, 55, 45, 43, 94, 32, 61, 67, 39, 34, 73, 69, 84, 25, 48, 31, 53, 53, 27, 54, 49, 107, 54, 76, 40, 74, 59, 106, 23, 39, 46, 71, 35, 89, 44, 37, 88, 63, 45, 23, 66, 69, 24, 44, 62, 29, 64, 37, 73, 69, 81, 113, 66, 49, 92, 20, 45, 62, 42, 75, 94, 34, 48, 29, 51, 60, 83, 46, 51, 46, 75, 80, 58, 52, 43, 82, 45, 32, 84, 94, 60, 44, 49, 61, 60, 48, 54, 120, 47, 62, 125, 32, 40, 82, 48, 103, 60, 59, 74, 55, 54, 62, 50, 42, 56, 36, 44, 55, 44, 29, 77, 36, 49, 117, 79, 23, 110, 72, 49, 87, 34, 42, 67, 55, 59, 37, 62, 95, 35, 44, 95, 55, 90, 38, 57, 35, 69, 53, 37, 91, 35, 138, 55, 53, 45, 54, 74, 58, 32,   178  , 49, 27, 35, 74, 51, 101, 45, 35, 53, 54, 99, 35, 83, 44]
      
   maintenant il faudrait que je génère une distribution de 1000 samples avec cette technique pour comparer avec la distribution q'uon essaye d'avoir (ça me semble bizarre car dans les faits pas vraiment ciblé à première vue, mais on verra bien)
    
    -avec le dpt 10 : un génération à 500 steps donne un truc globalement similaire à la distribution cftp -~ c'est cool
    
    
    Dans les dpts non connexe : on aura probablement le DPT 74 ( haute savoie) qui devrait avoir un truc comme .. CC
      Ca explique le fait que dans le tableau de génération en non pondéré on ait des découpages qui ont un grand nombre de st et qui ne soient jamais (ou quasi jamais) tirés
        NB : les deux dpts qui ne sont pas connexes actuellement sont des dpts qui ont un faible ratio nb_cantons/nb_circos (m^me si ça ne doit pas être le seul paramètre je pense)
        
   quel est le lien entre le nombre de steps necessaires pour converger avec cftp et le nombre d'itérations dont on a besoin pour avoir un sampling valable en utilisant Recom de manière classique ?

Rapport réunion vendredi 18 juillet 14h : 
  -je leur ai expliqué que le cftp donnait des departmeent non connexes (comme le 74 (haute savoie) et le 67 (Bas rhin)) a priori David a trouvé un article qui donne des résultats positifs sur la connexité (mais je ne l'ai pas reçu, problème de mail a priori) 
  -on aimerait avoir un résultat positif
  
  -on remarque que le département 74 met en pratique le contre exemple avec un triangle coupé bizarrement avec des poids étranges
  
  -on sait déja que la question suivante admet une réponse négative: 
    Existe t-il k (constante) tel que pour tout G planaire tq G et G* (dual de G) de degré borné et planaires alors la chaine de Markov qui merge K cantons et les re-sépare à chaque étape est connexe : 
    
      prendre mon exemple avec un cercle (cycle) de lg n avec des circos avec deux décupages différents un avec des kinda portes logiques "ou" dans le sens horaires et la meêm choe dans le sens anti-horaire, alors pour tout n il faudra merge n circos à chaque fois pour avoir un ens des découpages connexes (cf photo du tableau de david, il y a mon dessin)
      on pourrait se dire qu'on veut un paramètre comme nb cantons / nb_cicrcos grand mais ça ne résout pas le problème car on peut faire en sorte de mettre des trucs "dans" les circos, tq si on prends la circos a alors on est obligé de prendre toutes les circos dans la circos a (ie on a un bridge) (ie si on enlève le canton a on déconnece le graphe) -~ donc ce n'est pas si simple que ça 
      on pourrait se dire qu'il faut que tous les cantons aient à peu pres la même population, et la le probleme persiste toujours en faisant la même construction que ci dessus
      la solution qy'on pourrait trouver serait : on fait attention à la population de toutes les circos sauf *un nb*/ sauf *un pourcentage* et là tout est cassé (mais dans les faits ce n'est globalement pas le cas)
        - je pense qu'il peut etre intéressent d'avoir une contrainte de ne pas avoir de bridge et/ou de ne pas avoir de truc type donut (en pratique on aura jamais le truc de cycle de cantons avec rien au centre)
   -on s'est ensuite posé une question avec des zones rouges bizarre je n'ai pas tout compris (on était probablement sur des algos d'approx, je ne sais pas trop)
   -ensuite on voulait démontrer un résultat sur des graphes particuliers : 
     -si le graphe de connexité des cantons est une ligne, on peut démontrer la connexité (le faire!!!!!)
     -si le graphe de connexité des cantons est un arbre, alors .... il faudrait le faire, mais probablement pas le temps)
     
     
   dans les trucs à faire : 
     -faire la démo sur la ligne (ie quand le graphe de connexité est une ligne alors le graphe des différents découpages possibles est connexe)
       voir si on est dans un cas pondéré oy non (dans l'exemple au tableau non, mais voir si ça se généralise)
     -voir pour réduire la pente de la courbe en log de dans le cas non pondéré (avec mon truc de accepter avec une certaine proba)
     -écrire le rapport
     -nettoyer mon code pour le donner à David et Claire
     
        
        
        
   Prochaine réunion mercredi 14h mardi ??
   

    
    
   
  -
	
  
    
    
    
    
    
  POSSIBLE DE FAIRE : 
  -faire fonctionner opti quiutilise structure des aretes qui coupent
  -mettre des poids sur les aretes et développer l'algo pour s'en occuper (distance géodésique)
    -améliorer le code juila pour écrire les formes une seule fois et les colorier plutot que de les dessiner beaucoup de fois!
    -regarder ce que ça donne avec Kruskal (en non pondéré)
       -déssiner les graphes plutot que les département pour avoir une idée en regardant la topologie du graphe
   -possibilité de faire un tirage avec une idée de duplication d'arêtes proportionnelle à la taille de l'arête en question : ça nous donne un tirage selon la distribution du produit des poids (ie plus un arbre a un produit de ses poids élevés, plus il aura de chances d'être tiré)
   -regarder s'il y a des choses qui ne sont pas coupées ou si par le découpage avec recom (exemple : communautés de communes (fleuves ?))
   algo de kirchoff et theorème des matrices de ...pour calculer le nombre de spanning tree
     -idée selon laquelle le cas à deux circos dans un dpt doit être connu totalement, en tout cas la distribution
        -Problème de foret couvrante dans les territoires non connexes non? -~ je pense que les graphes sont faits pour résoudre ce problème
        -faire des graphiques (ex : l'inégalité, distibution en fonction du nombre de spanning tree)
